#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Sep 10 05:49:49 2024

@author: leszekwierzchleyski
"""

"""This script implements a CNN to classify X-Ray images of the brain as to 
whether or not a tumour is present. The images are fed through augmnetatation 
layers first in order to improve training. The code saves the weights of the 
best epoch for accuracy as well as the best epoch for false negatives."""

import tensorflow as tf
import keras
import matplotlib.pyplot as plt
import warnings




warnings.simplefilter("ignore", UserWarning)

"""Create Training and Validation image data sets. Many thanks to 
PREET VIRADIYA for the dataset """

File = "Brain_Tumor_Data_Set2"

train_ds = keras.utils.image_dataset_from_directory(
    File,
    labels="inferred",
    label_mode="int",
    class_names=None,
    color_mode="grayscale",
    batch_size=32,
    #image_size=(256, 256),
    shuffle=True,
    seed=1,
    validation_split=0.2,
    subset='training',
    interpolation="bilinear",
    follow_links=False
 
    
)

val_ds = keras.utils.image_dataset_from_directory(
    File,
    labels="inferred",
    label_mode="int",
    class_names=None,
    color_mode="grayscale",
    batch_size=32,
    #image_size=(256, 256),
    shuffle=True,
    seed=1,
    validation_split=0.2,
    subset='validation',
    interpolation="bilinear",
    follow_links=False
 
    
)



class_names = train_ds.class_names
print(class_names)

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"), cmap='gray')
    plt.title(class_names[labels[i]])
    plt.axis("off")
    
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)


"""Augmentation Layers"""

Normal = tf.keras.layers.Rescaling(1./1024)


Translate = keras.layers.RandomTranslation(
    height_factor=(-0.5,0.5),
    width_factor=(-0.5,0.5),
    fill_mode="reflect",
    interpolation="bilinear",
    seed=1,
    fill_value=0.0,
    trainable=True
    
    
)

Flip = keras.layers.RandomFlip(mode="horizontal_and_vertical", seed=None, trainable=True)

Rotation = keras.layers.RandomRotation(
    factor=(-0.2, 0.3),
    fill_mode="reflect",
    interpolation="bilinear",
    seed=None,
    fill_value=0.0,
    
    trainable=True
    
)

Zoom = keras.layers.RandomZoom(
    height_factor=(-0.5,0.5),
    width_factor=(-0.5,0.5),
    fill_mode="reflect",
    interpolation="bilinear",
    seed=None,
    fill_value=0.0,
    trainable=True
    
)


""""Network Layers"""


Pool = keras.layers.MaxPooling2D(
    pool_size=(2, 2), strides=None, padding="same"
)



Conv = keras.layers.Conv2D(
    filters=64,
    kernel_size=(7,7),
    strides=(2, 2),
    padding="same",
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation='leaky_relu',
    use_bias=True,
    kernel_initializer="he_normal",
    bias_initializer="zeros",
    kernel_regularizer='l1',
    bias_regularizer='l1',
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None
    
)



Flatten = keras.layers.Flatten()
Dense1 = keras.layers.Dense(50, activation='leaky_relu', kernel_initializer='he_normal')
Dense2 = keras.layers.Dense(20, activation='leaky_relu', kernel_initializer='he_normal')
Dense3 = keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_normal')

BatchNorm = keras.layers.BatchNormalization(
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer="zeros",
    gamma_initializer="ones",
    moving_mean_initializer="zeros",
    moving_variance_initializer="ones",
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None
   
    
)

Act = keras.layers.Activation('leaky_relu')





Dropout = keras.layers.Dropout(0.1)






"""Network Architecture"""

model = keras.Sequential()

model.add(Normal)

model.add(Translate)

model.add(Rotation)
model.add(Flip)
model.add(Zoom)



model.add(Conv)

model.add(Dropout)

model.add(Act)

model.add(BatchNorm)

model.add(Pool)



model.add(Conv)

model.add(Dropout)

model.add(Act)

model.add(BatchNorm)

model.add(Pool)

model.add(Conv)

model.add(Dropout)

model.add(Act)

model.add(BatchNorm)


model.add(Pool)

model.add(Conv)

model.add(Dropout)

model.add(Act)

model.add(BatchNorm)


model.add(Pool)



model.add(Flatten)

model.add(Dense1)

model.add(Dense2)

model.add(Dense3)


optimizer = keras.optimizers.Adam(
        learning_rate=1e-4,
        beta_1=0.9,
        beta_2=0.999,
        amsgrad=True
        
    )


"""Loss, Optimizer, and Checkpoint functions"""


loss = keras.losses.BinaryCrossentropy()

model.compile(optimizer=optimizer,
                  loss=loss,
                  metrics=['accuracy','FalseNegatives'])
checkpoint = tf.keras.callbacks.ModelCheckpoint(
    'Tumour_Weights/Best_Weights_acc.ckpt',
    monitor='val_accuracy',
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode='max',
    save_freq='epoch',
    initial_value_threshold=None
)

checkpoint2 = tf.keras.callbacks.ModelCheckpoint(
    'Tumour_Weights/Best_Weights_FN.ckpt',
    monitor='val_false_negatives',
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode='min',
    save_freq='epoch',
    initial_value_threshold=None
)

model.fit(train_ds, validation_data=val_ds, batch_size=32, epochs=500, callbacks=[checkpoint, checkpoint2], verbose=True)

